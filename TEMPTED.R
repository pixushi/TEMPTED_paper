library(ggplot2)
library(np)


#' @title Decomposition of temporal tensor
#' @description This is the main function of tempted.
#' @param datlist A length n list of matrices.
#' Each matrix represents a subject,
#' with columns representing samples from this subject,
#' the first row representing the sampling time points,
#' and the following rows representing the feature values.
#' @param r Number of components to decompose into, i.e. rank of the CP type decomposition.
#' Default is set to 3.
#' @param smooth Smoothing parameter for RKHS norm.
#' Larger means smoother temporal loading functions. Default is set to be 1e-8.
#' Value can be adjusted depending on the dataset by checking the smoothness of the estimated temporal loading function in plot.
#' @param interval The range of time points to ran the decomposition for.
#' Default is set to be the range of all observed time points.
#' User can set it to be a shorter interval than the observed range.
#' @param resolution Number of time points to evaluate the value of the temporal loading function.
#' Default is set to 101. It does not affect the subject or feature loadings.
#' @param maxiter Maximum number of iteration. Default is 20.
#' @param epsilon Convergence criteria for difference between iterations. Default is 1e-4.
#' @return The estimations of the loadings.
#' \item{A.hat}{Subject loading, a subject by r matrix.}
#' \item{B.hat}{Feature loading, a feature by r matrix.}
#' \item{Phi.hat}{Temporal loading function, a resolution by r matrix.}
#' \item{time.Phi}{The time points where the temporal loading function is evaluted.}
#' \item{Lambda}{Eigen value, a length r vector.}
#' \item{r.square}{Variance explained by each component.
#' This is the R-squared of the linear regression of the vectorized temporal tensor against the vectorized low-rank reconstruction using individual components.}
#' \item{accum.r.square}{Variance explained by the first few components accumulated.
#' This is the R-squared of the linear regression of the vectorized temporal tensor against the vectorized low-rank reconstruction using the first few components.}
#' @examples
#' # for count data from longitudinal microbiome studies
#'
#' datlist <- format_tempted(count_table,
#'                           meta_table$day_of_life,
#'                           meta_table$studyid,
#'                           pseudo_count=0.5,
#'                           transform="clr")
#'
#' mean_svd <- svd_centralize(datlist, r=1)
#'
#' res_tempted <- tempted(mean_svd$datlist, r=3, smooth=1e-5)
#'
#' # for preprocessed data that do not need to be transformed
#'
#' datlist <- format_tempted(processed_table,
#'                           meta_table$day_of_life,
#'                           meta_table$studyid,
#'                           pseudo_count=NULL,
#'                           transform="none")
#'
#' mean_svd <- svd_centralize(datlist, r=1)
#'
#' res_tempted <- tempted(mean_svd$datlist, r=3, smooth=1e-5)
#'
#' # plot the temporal loading
#'
#' plot_time_loading(res_tempted, r=2)
#' @md
tempted <- function(datlist, r = 3, smooth=1e-6,
                    interval = NULL, resolution = 101,
                    maxiter=20, epsilon=1e-4){
  n <- length(datlist)
  p <- nrow(datlist[[1]])-1

  Lambda <- rep(0, r)
  A <- matrix(0, n, r)
  B <- matrix(0, p, r)
  Phi <- matrix(0, resolution, r)
  PCname <- paste('Component', 1:r)
  colnames(A) <- PCname
  colnames(B) <- PCname
  colnames(Phi) <- PCname
  rownames(A) <- names(datlist)
  rownames(B) <- rownames(datlist[[1]])[-1]


  # Calculate range.
  timestamps.all <- do.call(c,lapply(datlist, FUN=function(u){u[1,]}))

  timestamps.all <- sort(unique(timestamps.all))
  if (is.null(interval)){
    interval <- c(timestamps.all[1], timestamps.all[length(timestamps.all)])
  }

  # rescale the time to 0-1.
  input.time.range <- c(timestamps.all[1], timestamps.all[length(timestamps.all)])
  for (i in 1:n){
    datlist[[i]][1,] <- (datlist[[i]][1,] - input.time.range[1]) / (input.time.range[2] - input.time.range[1])
  }
  interval <- (interval - input.time.range[1]) / (input.time.range[2] - input.time.range[1])

  res <- NULL
  Lambda <- rep(0, r)
  X <- NULL
  y0 <- NULL
  Rsq <- accumRsq <- rep(0, r)

  ti <- vector(mode='list', length=n)
  for (i in 1:n){
    temp <- 1 + round((resolution-1) * (datlist[[i]][1,] - interval[1]) / (interval[2] - interval[1]))
    temp[which(temp<=0 | temp>resolution)] <- 0
    ti[[i]] <- temp
  }

  tipos <- vector(mode='list', length=n)
  for (i in 1:n){
    keep <- ti[[i]]>0
    tipos[[i]] <- keep
    y0 <- c(y0, as.vector(t(datlist[[i]][2:(p+1),keep])))
  }

  Lt <- list()
  ind_vec <- NULL
  for (i in 1:n){
    Lt <- c(Lt, list(datlist[[i]][1,]))
    ind_vec <- c(ind_vec, rep(i,length(Lt[[i]])))
  }

  tm <- unlist(Lt)
  Kmat <- bernoulli_kernel(tm, tm)
  Kmat_output <- bernoulli_kernel(seq(interval[1],interval[2],length.out = resolution), tm)

  # calculate rank-1 component sequentially.
  for (s in 1:r){
    # Step 1: initialization.
    print(sprintf("Calculate the %dth Component", s))

    # intialization of b
    data.unfold = NULL
    y <- NULL
    for (i in 1:n){
      data.unfold = cbind(data.unfold, datlist[[i]][2:(p+1),])
      y <- c(y, as.vector(t(datlist[[i]][2:(p+1),tipos[[i]]])))
    }
    b.initials <- svd(data.unfold, nu=r, nv=r)$u
    b.hat <- b.initials[,1]
    # initialization of a
    a.hat <- rep(1,n)/sqrt(n)

    # iteratively update a,b,phi
    t <- 0
    dif <- 1
    while(t<=maxiter & dif>epsilon){
      # update phi:
      Ly <- list()
      for (i in 1:n){
        Ly <- c(Ly, list(a.hat[i]*as.numeric(b.hat%*%datlist[[i]][2:(p+1),])))
      }
      phi.hat <- freg_rkhs(Ly, a.hat, ind_vec, Kmat, Kmat_output, smooth=smooth)
      phi.hat <- phi.hat / sqrt(sum(phi.hat^2))

      # update a:
      a.tilde <- rep(0,n)
      for (i in 1:n){
        t.temp <- tipos[[i]]
        a.tilde[i] <- b.hat %*% datlist[[i]][2:(p+1),t.temp] %*% phi.hat[ti[[i]][t.temp]]
        a.tilde[i] <- a.tilde[i] / sum((phi.hat[ti[[i]][t.temp]])^2)
      }
      a.new <- a.tilde / sqrt(sum(a.tilde^2))
      dif <- sum((a.hat - a.new)^2)
      a.hat <- a.new

      # update b:
      temp.num <- matrix(0,p,n)
      temp.denom <- rep(0,n)
      for (i in 1:n){
        t.temp <- tipos[[i]]
        temp.num[,i] <- datlist[[i]][2:(p+1),t.temp] %*% phi.hat[ti[[i]][t.temp]]
        temp.denom[i] <-sum((phi.hat[ti[[i]][t.temp]])^2)
      }
      b.tilde <- as.numeric(temp.num%*%a.hat) / as.numeric(temp.denom%*%(a.hat^2))
      b.new <- b.tilde / sqrt(sum(b.tilde^2))
      dif <- max(dif, sum((b.hat - b.new)^2))
      b.hat <- b.new

      t <- t+1
    }

    # calculate lambda
    x <- NULL
    for (i in 1:n){
      t.temp <- ti[[i]]
      t.temp <- t.temp[t.temp>0]
      x <- c(x,as.vector(t(a.hat[i]*b.hat%o%phi.hat[t.temp])))
    }
    X <- cbind(X, x)
    l.fit <- lm(y~x-1)
    lambda <- as.numeric(l.fit$coefficients)
    A[,s] <- a.hat
    B[,s] <- b.hat
    Phi[,s] <- t(phi.hat)
    Lambda[s] <- lambda
    Rsq[s] <- summary(l.fit)$r.squared
    accumRsq[s] <- summary(lm(y0~X-1))$r.squared

    # update datlist
    for (i in 1:n){
      temp <- tipos[[i]]
      datlist[[i]][2:(p+1),which(temp)] <- datlist[[i]][2:(p+1),which(temp)] -
        Lambda[s] * A[i,s] * (B[,s] %*% t(Phi[ti[[i]][temp],s]))
    }
    print(paste0("Convergence reached at dif=", dif, ', iter=', t))
  }
  l.fit <- lm(y0~X-1)
  Lambda <- as.numeric(l.fit$coefficients)

  # revise the sign of Lambda
  for (r in 1:length(Lambda)){
    if (Lambda[r]<0){
      Lambda[r] <- -Lambda[r]
      A[,r] <- -A[,r]
    }
  }
  # revise the signs to make sure summation of phi is nonnegative
  sgn_phi <- sign(colSums(Phi))
  sgn_phi[sgn_phi==0] <- 1
  for (r in 1:ncol(Phi)){
    Phi[,r] <- sgn_phi[r]*Phi[,r]
    A[,r] <- sgn_phi[r]*A[,r]
  }
  # revise the signs to make sure summation of B is nonnegative
  sgn_B <- sign(colSums(B))
  sgn_B[sgn_B==0] <- 1
  for (r in 1:ncol(Phi)){
    B[,r] <- sgn_B[r]*B[,r]
    A[,r] <- sgn_B[r]*A[,r]
  }
  time.return <- seq(interval[1],interval[2],length.out = resolution)
  time.return <- time.return * (input.time.range[2] - input.time.range[1]) + input.time.range[1]
  results <- list("A.hat" = A, "B.hat" = B,
                 "Phi.hat" = Phi, "time.Phi" = time.return,
                 "Lambda" = Lambda, "r.square" = Rsq, "accum.r.square" = accumRsq)
  return(results)
}


#' @title Remove the mean structure of the temporal tensor
#' @description This function first average the feature value of all time points for each subject to form a subject by feature matrix.
#' Next, it performs a singular value decomposition of this matrix and construct the matrix's rank-r approximation.
#' Then, it subtracts this rank-r subject by feature matrix from the temporal tensor.
#' @param datlist A length n list of matrices.
#' Each matrix represents a subject,
#' with columns representing samples from this subject,
#' the first row representing the sampling time points,
#' and the following rows representing the feature values.
#' @param r The number of ranks in the mean structure. Default is 1.
#' @return A list of results.
#' \item{datlist}{The new temporal tensor after mean structure is removed}
#' \item{A.tilde}{The subject singular vector of the mean structure, a subject by r matrix.}
#' \item{B.tilde}{The feature singular vector of the mean structure, a feature by r matrix.}
#' \item{lambda.tilde}{The singular value of the mean structure, a length r vector.}
#' @seealso Examples can be found in \code{\link{tempted}}.
#' @md
svd_centralize <- function(datlist, r = 1){
  n <- length(datlist)
  p <- nrow(datlist[[1]])-1
  mean_mat <- matrix(0,n,p)
  for (i in 1:length(datlist)){
    mean_mat[i,] <- apply(datlist[[i]][-1,], 1, mean)
  }
  mean_mat.svd <- svd(mean_mat, nu=r, nv=r)
  mean_mat.svd1 <- mean_mat.svd$u %*% t(mean_mat.svd$v * mean_mat.svd$d[1:r])
  mf.new <- datlist
  for (i in 1:length(datlist)){
    mf.new[[i]][-1,] <- datlist[[i]][-1,] - mean_mat.svd1[i,]
  }
  results <- list("datlist" = mf.new, "A.tilde" = mean_mat.svd$u,
                  "B.tilde" = mean_mat.svd$v, "lambda.tilde" = mean_mat.svd$d[1:r])
  return(results)
}


#' @title Format data table into the input of tempted
#' @description This function applies a variety of transformations to the read counts and
#' format the sample by feature table and meta data into a data list
#' that can be used as the input of \code{\link{tempted}} and \code{\link{svd_centralize}}.
#' For data that are not read counts, or data that are not microbiome data,
#' the user can apply their desired transformation to the data before formatting into list.
#' @param feature_table A sample by feature matrix.
#' @param time_point The time stamp of each sample, matched with the rows of \code{feature_table}.
#' @param subjectID The subject ID of each sample, matched with the rows of \code{feature_table}.
#' @param threshold A threshold for feature filtering for microbiome data.
#' Features with zero value percentage > threshold will be excluded. Default is 0.95.
#' @param pseudo_count A small number to add to all the counts before
#' normalizing into proportions and log transformation.
#' Default is 1/2 of the smallest non-zero value that is specific for each sample.
#' This pseudo count is added for \code{transform=c("log_comp", "clr", "logit")}.
#' @param transform The transformation applied to the data.
#' \code{"log_comp"} for log of compositions.
#' \code{"comp"} for compositions.
#' \code{"ast"} for arcsine squared transformation.
#' \code{"clr"} for central log ratio transformation.
#' \code{"logit"} for logit transformation.
#' \code{"none"} for no transformation.
#' Default \code{transform="clr"} is recommended for microbiome data.
#' For data that are already transformed, use \code{transform="none"}.
#' @return A length n list of matrices as the input of \code{\link{tempted}} and \code{\link{svd_centralize}}.
#' Each matrix represents a subject,
#' with columns representing samples from this subject,
#' the first row representing the sampling time points,
#' and the following rows representing the feature values.
#' @seealso Examples can be found in \code{\link{tempted}}.
#' @md
format_tempted <- function(feature_table, time_point, subjectID,
                           threshold=0.95, pseudo_count=NULL, transform="clr"){
  ntm <- which(table(subjectID)==1)
  if(length(ntm)>0)
    stop(paste('Please remove these subjects with only one time point:',
               paste(names(ntm), collapse=', ')))
  if (length(subjectID)!=nrow(feature_table))
    stop('length of subjectID does not match feature_table!')
  if (length(time_point)!=nrow(feature_table))
    stop('length of time_point does not match feature_table!')
  # get pseudo count
  if (is.null(pseudo_count) & (transform %in% c("clr", "log_comp", "logit"))){
    pseudo_count <- apply(feature_table, 1, function(x){
      min(x[x!=0])/2
    })
  }
  # keep taxon that has non-zeros in >1-threshold samples
  feature_table <- feature_table[,colMeans(feature_table==0)<=threshold]
  if(transform=='log_comp'){
    feature_table <- feature_table+pseudo_count
    feature_table <- t(log(feature_table/rowSums(feature_table)))
  }else if(transform=='comp'){
    feature_table <- feature_table
    feature_table <- t(feature_table/rowSums(feature_table))
  }else if(transform=='ast'){
    feature_table <- feature_table
    feature_table <- t(asin(sqrt(feature_table/rowSums(feature_table))))
  }else if(transform=='clr'){
    feature_table <- feature_table+pseudo_count
    feature_table <- log(feature_table/rowSums(feature_table))
    feature_table <- t(feature_table-rowMeans(feature_table))
  }else if(transform=='logit'){
    feature_table <- feature_table+pseudo_count
    feature_table <- t(feature_table/rowSums(feature_table))
    feature_table <- log(feature_table/(1-feature_table))
  }else if(transform=='none'){
    feature_table <- t(feature_table)
  }else{
    print('Input transformation method is wrong! log_comp is applied instead')
    feature_table <- feature_table+pseudo_count
    feature_table <- t(log(feature_table/rowSums(feature_table)))
  }
  feature_table <- rbind(time_point, feature_table)
  rownames(feature_table)[1] <- 'time_point'
  subID <- unique(subjectID)
  nsub <- length(subID)

  # construct list of data matrices, each element representing one subject
  datlist <- vector("list", length = nsub)
  names(datlist) <- subID

  # Each slice represents an individual (unequal sized matrix).
  for (i in 1:nsub){
    # print(i)
    datlist[[i]] <- feature_table[, subjectID==subID[i]]
    datlist[[i]] <- datlist[[i]][,order(datlist[[i]][1,])]
    datlist[[i]] <- datlist[[i]][,!duplicated(datlist[[i]][1,])]
  }
  return(datlist)
}




#' @title Caculate the Bernoulli kernel
#' @description This function is used to calculate the kernel matrix for the
#' RKHS regression that iteratively updates the temporal loading function.
#' @param x,y Two values between which the Bernoulli kernel is calculated.
#' @return The calculated kernel between \code{x} and \code{y}.
#' @md
bernoulli_kernel <- function(x, y){
  k1.x <- x-0.5
  k1.y <- y-0.5
  k2.x <- 0.5*(k1.x^2-1/12)
  k2.y <- 0.5*(k1.y^2-1/12)
  xy <- abs(x %*% t(rep(1,length(y))) - rep(1,length(x)) %*% t(y))
  k4.xy <- 1/24 * ((xy-0.5)^4 - 0.5*(xy-0.5)^2 + 7/240)
  kern.xy <- k1.x %*% t(k1.y) + k2.x %*% t(k2.y) - k4.xy + 1
  return(kern.xy)
}



#' RKHS regression to update the temporal loading function.
#' @description This function is an internal function that performs the RKHS regression.
#' It is applied iteratively in \code{\link{tempted}} to update the temporal loading function.
#' @md
freg_rkhs <- function(Ly, a.hat, ind_vec, Kmat, Kmat_output, smooth=1e-8){
  A <- Kmat
  for (i in 1:length(Ly)){
    A[ind_vec==i,] <- A[ind_vec==i,]*a.hat[i]^2
  }
  cvec <- unlist(Ly)

  A.temp <- A + smooth*diag(ncol(A))
  beta <- solve(A.temp)%*%cvec

  phi.est <- Kmat_output %*% beta
  return(phi.est)
}


#' @title Calculate the de-noised temporal tensor
#' @description This function constructs a de-noised version of the temporal tensor
#' using the low-rank components obtained by \code{\link{svd_centralize}} \code{\link{tempted}} and uses the loadings to
#' @param res_tempted Output of tempted
#' @param mean_svd Output of svd_centralize
#' @return The de-noised functional tensor
#' @md
tdenoise <- function(res_tempted, mean_svd=NULL){
  n <- nrow(res_tempted$A.hat)
  p <- nrow(res_tempted$B.hat)
  resol <- nrow(res_tempted$Phi.hat)
  tensor.est <- array(0,dim=c(n,p,resol))
  if (!is.null(mean_svd))
    tensor.est <- (mean_svd$A.tilde %*% t(mean_svd$B.tilde * mean_svd$lambda.tilde)) %o%
    rep(1, resol)
  for (i in 1:ncol(res_tempted$A.hat)){
    tensor.est <- tensor.est+res_tempted$A.hat[,i]%o%res_tempted$B.hat[,i]%o%res_tempted$Phi.hat[,i]*res_tempted$Lambda[i]
  }
  dimnames(tensor.est)[[3]] <- res_tempted$time.Phi
  return(tensor.est)
}


#' @title Estimate subject loading of testing data
#' @description This function estimates the subject loading of the testing data
#' based on feature and temporal loading from training data,
#' so that both the testing data and training data have the same dimensionality reduction.
#' @param datlist Testing data formatted into datlist in the same fashion as the training data.
#' The same transformation needs to be used for both training and testing data.
#' @param res_tempted Result from \code{\link{tempted}} ran on the training data.
#' @param mean_svd Result from \code{\link{svd_centralize}} ran on the training data.
#' @return estimated subject loading of testing data
#' @examples
#' # split the example data into training and testing
#'
#' id_test <- meta_table$studyid=="1"
#'
#' count_train <- count_table[!id_test,]
#' meta_train <- meta_table[!id_test,]
#'
#' count_test <- count_table[id_test,]
#' meta_test <- meta_table[id_test,]
#'
#' # run tempted on training data
#'
#' datlist_train <- format_tempted(count_train,
#'                                 meta_train$day_of_life,
#'                                 meta_train$studyid,
#'                                 threshold=0.95,
#'                                 pseudo_count=0.5,
#'                                 transform="clr")
#'
#' mean_svd_train <- svd_centralize(datlist_train, r=1)
#'
#' res_tempted_train <- tempted(mean_svd_train$datlist,
#' r=3, smooth=1e-5)
#'
#' # get the overlapping features
#'
#' count_test <- count_test[,rownames(datlist_train[[1]])[-1]]
#'
#' datlist_test <- format_tempted(count_test,
#'                                meta_test$day_of_life,
#'                                meta_test$studyid,
#'                                threshold=1,
#'                                pseudo_count=0.5,
#'                                transform="clr")
#'
#' # estimate the subject loading of the testing subject
#'
#' sub_test <- est_test_subject(datlist_test, res_tempted_train, mean_svd_train)
#' @md
est_test_subject <- function(datlist, res_tempted, mean_svd=NULL){
  B <- res_tempted$B.hat
  Phi <- res_tempted$Phi.hat
  Lambda <- res_tempted$Lambda
  time.return <- res_tempted$time.Phi
  n <- length(datlist)
  p <- nrow(B)
  r <- ncol(B)
  resolution <- length(time.return)
  A_test <- matrix(0,n,r)
  y <- NULL
  ti <- vector(mode = "list", length = n)
  # get the coordinate of observed time points in the returned time grid
  for (i in 1:n){
    ti[[i]] <- sapply(datlist[[i]][1,], function(x){which.min(abs(x-time.return))})
    y <- c(y, as.numeric(t(datlist[[i]][-1,ti[[i]]>0])))
  }
  mf.new <- datlist
  if(!is.null(mean_svd)){
    mean_mat <- matrix(0,n,p)
    for (i in 1:length(datlist)){
      mean_mat[i,] <- apply(datlist[[i]][-1,], 1, mean)
    }
    mean_mat.svd1 <- mean_mat%*%tcrossprod(mean_svd$B.tilde)
    mf.new <- datlist
    for (i in 1:length(datlist)){
      mf.new[[i]][-1,] <- datlist[[i]][-1,] - mean_mat.svd1[i,]
    }
  }

  for (s in 1:r){
    for (i in 1:n){
      t.temp <- ti[[i]]>0
      A_test[i,s] <- B[,s] %*% mf.new[[i]][2:(p+1),t.temp] %*% Phi[ti[[i]][t.temp],s]
      A_test[i,s] <- A_test[i,s] / sum((Phi[ti[[i]][t.temp],s])^2) / Lambda[s]
      mf.new[[i]][2:(p+1),t.temp] <- mf.new[[i]][2:(p+1),t.temp] -
        Lambda[s] * A_test[i,s] * (B[,s] %*% t(Phi[ti[[i]][t.temp],s]))
    }
  }
  rownames(A_test) <- names(mf.new)
  colnames(A_test) <- paste('Component', 1:r)
  return(A_test)
}


#' @title Aggregate features using feature loadings
#' @description This function aggregate the features into "meta features" by
#' calculating a weighted summation of the features using feature loading of each component as weights.
#' It can also aggregate features by using the combination of multiple components by ranking the features
#' by a linear combination of feature loadings from multiple components.
#' @param res_tempted Output of \code{\link{tempted}}.
#' @param mean_svd Output of \code{\link{svd_centralize}}.
#' @param datlist Output of \code{\link{format_tempted}}, the original temporal tensor that will be aggregated.
#' @param pct The percent of features to aggregate,
#' features ranked by absolute value of the feature loading of each component.
#' Default is 1, which means 100% of features are aggregated.
#' Setting \code{pct=0.01} means top 1% of features is aggregated,
#' where features are ranked in absolute value of feature loading of each component.
#' @param contrast A matrix choosing how components are combined,
#' each column is a contrast of length r and used to calculate the linear combination of
#' the feature loadings of r components.
#' @param get_contrast A vector denoting which components to use to construct a contrast and combine.
#' A vector of \code{c(1,1,0)} means the first two of three components are used to find the contrast to combine.
#' @return A list of results.
#' \item{metafeature.aggregate}{The meta feature obtained by aggregating the observed temporal tensor.
#' It is a data.frame with four columns: "value" for the meta feature values,
#' "subID" for the subject ID, "timepoint" for the time points,
#' and "PC" indicating which component was used to construct the meta feature.}
#' \item{metafeature.aggregate.est}{The meta feature obtained by aggregating the denoised temporal tensor.
#' It has the same structure as \code{metafeature.aggregate}.}
#' \item{contrast}{The contrast used to linearly combine the components.
#' It is either from the input parameter \code{contrast} or calculated by setting \code{get_contrast}.}
#' \item{toppct}{A matrix of TRUE/FALSE indicating which features are aggregated in each component and contrast.}
#' @examples
#' datlist <- format_tempted(count_table,
#'                           meta_table$day_of_life,
#'                           meta_table$studyid,
#'                           pseudo_count=0.5,
#'                           transform="clr")
#'
#' mean_svd <- svd_centralize(datlist, r=1)
#'
#' res_tempted <- tempted(mean_svd$datlist, r=3, smooth=1e-5)
#'
#' datlist_raw <- format_tempted(count_table,
#'                               meta_table$day_of_life,
#'                               meta_table$studyid,
#'                               transform="none")
#'
#' contrast <- cbind(c(1/2,1,0), c(1/2,-1,0))
#'
#' res_aggregate <- aggregate_feature(res_tempted,
#'                                    mean_svd,
#'                                    datlist,
#'                                    pct=1,
#'                                    contrast=contrast)
#'
#' group <- unique(meta_table[, c("studyid", "delivery")])
#'
#' plot_metafeature(res_aggregate$metafeature.aggregate, group, bws=30)
#' @md
aggregate_feature <- function(res_tempted, mean_svd=NULL, datlist,
                               pct=1,
                              contrast=NULL, get_contrast=NULL){
  B.data <- as.data.frame(res_tempted$B.hat)
  r <- ncol(B.data)
  if (!is.null(get_contrast)){
    datlist.agg <- sapply(datlist, function(x){t(B.data)%*%x[-1,]}, simplify=F)
    metafeature.aggregate <- NULL
    for (i in 1:length(datlist.agg)){
      tmp <- data.frame(value=as.vector(datlist.agg[[i]]),
                        subID=names(datlist.agg)[i],
                        timepoint=as.vector(t(matrix(datlist[[i]][1,], ncol(datlist[[i]]), ncol(B.data)))),
                        PC=rep(rownames(datlist.agg[[i]]), ncol(datlist.agg[[i]])))

      metafeature.aggregate <- rbind(metafeature.aggregate, tmp)
    }
    metafeature.aggregate.wide <- reshape(metafeature.aggregate, timevar="PC",
                                    idvar=c("subID", "timepoint"),
                                    direction="wide")
    colnames(metafeature.aggregate.wide)[-c(1,2)] <- substring(colnames(metafeature.aggregate.wide)[-c(1,2)],7)
    scale_mat <- scale(metafeature.aggregate.wide[-c(1,2)][get_contrast])
    component_svd <- svd(scale_mat)
    contrast_svd <- matrix(0, r, ncol(component_svd$v))
    contrast_svd[get_contrast,] <- component_svd$v
    contrast <- cbind(contrast, contrast_svd)
  }
  if(!is.null(contrast)){
    contrast.data <- res_tempted$B.hat%*%contrast
    colnames(contrast.data) <- paste('Contrast', 1:ncol(contrast))
    B.data <- cbind(B.data, contrast.data)
  }
  toppct <- apply(abs(B.data), 2, function(x){x>=quantile(x, 1-pct)})
  datlist.agg <- sapply(datlist, function(x){t(B.data*toppct)%*%x[-1,]}, simplify=F)
  metafeature.aggregate <- NULL
  for (i in 1:length(datlist.agg)){
    tmp <- data.frame(value=as.vector(datlist.agg[[i]]),
                      subID=names(datlist.agg)[i],
                      timepoint=as.vector(t(matrix(datlist[[i]][1,], ncol(datlist[[i]]), ncol(B.data)))),
                      PC=rep(rownames(datlist.agg[[i]]), ncol(datlist.agg[[i]])))

    metafeature.aggregate <- rbind(metafeature.aggregate, tmp)
  }
  metafeature.aggregate <- metafeature.aggregate[,c("value", "subID", "timepoint", "PC")]

  # estimated
  tensor.est <- tdenoise(res_tempted, mean_svd)
  tensor.est.agg <- apply(tensor.est, c(1,3), function(x){(t(B.data*toppct)%*%x)})
  metafeature.aggregate.est <- NULL
  for (i in 1:r){
    tmp <- data.frame(value=as.vector(tensor.est.agg[i,,]),
                      subID=rep(dimnames(tensor.est.agg)[[2]], dim(tensor.est.agg)[3]),
                      timepoint=as.vector(t(matrix(res_tempted$time.Phi, length(res_tempted$time.Phi),dim(tensor.est.agg)[2]))),
                      PC=colnames(B.data)[i])
    metafeature.aggregate.est <- rbind(metafeature.aggregate.est,tmp)
  }
  metafeature.aggregate.est <- metafeature.aggregate.est[,c("value", "subID", "timepoint", "PC")]
  metafeature.aggregate.est$type <- 'estimated'

  return(list(metafeature.aggregate=metafeature.aggregate,
              metafeature.aggregate.est=metafeature.aggregate.est,
              contrast=contrast,
              toppct=toppct))
}


#' @title Take log ratio of the abundance of top features over bottom features
#' @description Top and bottom ranking features are picked based on feature loadings (and their contrasts).
#' The log ratio abundance of the top ranking features over the bottom ranking features is produced as the main result.
#' This function and its result is designed for longitudinal microbiome data,
#' and may not be meaningful for other type of temporal data.
#' @param res_tempted Output of \code{\link{tempted}}.
#' @param datlist Output of \code{format_tempted(, transform="none")}, the temporal tensor that include the raw read counts.
#' @param absolute \code{absolute = TRUE} means features are ranked by the absolute value of feature loadings,
#' and the top \code{pct} percent of features are picked.
#' \code{absolute = FALSE} means features are ranked by the original value of feature loadings,
#' and the top and bottom \code{pct} percent of features are picked.
#' Then ratio is taken as the abundance of the features with positive loading
#' over the abundance of the features with negative loading.
#' @param pct The percent of features to sum up. Default is 0.05, i.e. 5%.
#' @param contrast A matrix choosing how components are combined,
#' each column is a contrast of length r and used to calculate the linear combination of
#' the feature loadings of r components.
#' @param get_contrast A vector denoting which components to use to construct a contrast and combine.
#' A vector of \code{c(1,1,0)} means the first two of three components are used to find the contrast to combine.
#' @return A list of results.
#' \item{metafeature.ratio}{The log ratio abundance of the top over bottom ranking features.
#' It is a data.frame with five columns: "value" for the log ratio values,
#' "subID" for the subject ID, and "timepoint" for the time points,
#' and "PC" indicating which component was used to construct the meta feature.}
#' \item{contrast}{The contrast used to linearly combine the components.
#' It is either from the input parameter \code{contrast} or calculated by setting \code{get_contrast}.}
#' \item{toppct}{A matrix of TRUE/FALSE indicating which features are ranked top in each component (and contrast)
#' and used as the numerator of the log ratio.}
#' \item{bottompct}{A matrix of TRUE/FALSE indicating which features are ranked bottom in each component (and contrast)
#' and used as the denominator of the log ratio.}
#' @examples
#' datlist <- format_tempted(count_table,
#'                           meta_table$day_of_life,
#'                           meta_table$studyid,
#'                           pseudo_count=0.5,
#'                           transform="clr")
#'
#' mean_svd <- svd_centralize(datlist, r=1)
#'
#' res_tempted <- tempted(mean_svd$datlist, r=3, smooth=1e-5)
#'
#' datlist_raw <- format_tempted(count_table, meta_table$day_of_life, meta_table$studyid,
#' transform="none")
#'
#' contrast <- cbind(c(1,1,0), c(1,-1,0))
#'
#' res_ratio <- ratio_feature(res_tempted, datlist_raw, pct=0.1,
#' absolute=FALSE, contrast=contrast)
#'
#' group <- unique(meta_table[, c("studyid", "delivery")])
#'
#' plot_metafeature(res_ratio$metafeature.ratio, group, bws=30)
#' @md
ratio_feature <- function(res_tempted, datlist,
                              pct=0.05, absolute=FALSE, contrast=NULL){
  B.data <- as.data.frame(res_tempted$B.hat)
  if (!is.null(contrast)){
    contrast.data <- res_tempted$B.hat%*%contrast
    colnames(contrast.data) <- paste('Contrast', 1:ncol(contrast))
    B.data <- cbind(B.data, contrast.data)
  }
  if(!absolute){
    toppct <- apply(B.data, 2, function(x){x>quantile(x, 1-pct) & x>0})
    bottompct <- apply(-B.data, 2, function(x){x>quantile(x, 1-pct) & x>0})
  }else{
    toppct <- apply(B.data, 2, function(x){abs(x)>quantile(abs(x), 1-pct) & x>0})
    bottompct <- apply(B.data, 2, function(x){abs(x)>quantile(abs(x), 1-pct) & x<0})
  }
  pseudo_count <- min(sapply(datlist, function(x){
    y<-x[-1,]
    return(min(y[y!=0]))
    }))/2
  datlist.ratio <- sapply(datlist,
                          function(x){
                            tt <- (t(toppct)%*%x[-1,])
                            bb <- (t(bottompct)%*%x[-1,])
                            return(log((tt+pseudo_count)/(bb+pseudo_count)))},
                          simplify=F)
  metafeature.ratio <- NULL
  for (i in 1:length(datlist.ratio)){
    tmp <- data.frame(value=as.vector(datlist.ratio[[i]]),
                      subID=names(datlist.ratio)[i],
                      timepoint=as.vector(t(matrix(datlist[[i]][1,], ncol(datlist[[i]]), ncol(B.data)))),
                      PC=rep(rownames(datlist.ratio[[i]]), ncol(datlist.ratio[[i]])))

    metafeature.ratio <- rbind(metafeature.ratio, tmp)
  }
  metafeature.ratio <- metafeature.ratio[,c("value", "subID", "timepoint", "PC")]
  return(list(metafeature.ratio=metafeature.ratio,
              contrast=contrast,
              toppct=toppct, bottompct=bottompct))
}

#' @title Run all major functions of tempted
#' @description This function wraps \code{\link{format_tempted}}, \code{\link{svd_centralize}}, \code{\link{tempted}},
#' \code{\link{ratio_feature}}, and \code{\link{aggregate_feature}}.
#' @param r Number of components to decompose into, i.e. rank of the CP type decomposition.
#' Default is set to 3.
#' @param smooth Smoothing parameter for RKHS norm.
#' Larger means smoother temporal loading functions. Default is set to be 1e-8.
#' Value can be adjusted depending on the dataset by checking the smoothness of the estimated temporal loading function in plot.
#' @param feature_table A sample by feature matrix. Input for \code{\link{format_tempted}}.
#' @param time_point The time stamp of each sample, matched with the rows of \code{feature_table}. Input for \code{\link{format_tempted}}.
#' @param subjectID The subject ID of each sample, matched with the rows of \code{feature_table}. Input for \code{\link{format_tempted}}.
#' @param threshold A threshold for feature filtering for microbiome data.
#' Features with zero value percentage >= threshold will be excluded. Default is 0.95.
#' Input for \code{\link{format_tempted}}.
#' @param pseudo_count A small number to add to all the counts before
#' normalizing into proportions and log transformation.
#' Default is 1/2 of the smallest non-zero value that is specific for each sample.
#' This pseudo count is added for \code{transform=c("log_comp", "clr", "logit")}.
#' Input for \code{\link{format_tempted}}.
#' @param transform The transformation applied to the data.
#' \code{"log_comp"} for log of compositions.
#' \code{"comp"} for compositions.
#' \code{"ast"} for arcsine squared transformation.
#' \code{"clr"} for central log ratio transformation.
#' \code{"logit"} for logit transformation.
#' \code{"none"} for no transformation.
#' Default \code{transform="clr"} is recommended for microbiome data.
#' For data that are already transformed, use \code{transform="none"}.
#' Input for \code{\link{format_tempted}}.
#' @param r Number of components to decompose into, i.e. rank of the CP type decomposition.
#' Default is set to 3.
#' Input for \code{\link{tempted}}.
#' @param smooth Smoothing parameter for RKHS norm.
#' Larger means smoother temporal loading functions. Default is set to be 1e-8.
#' Value can be adjusted depending on the dataset by checking the smoothness of the estimated temporal loading function in plot.
#' Input for \code{\link{tempted}}.
#' @param interval The range of time points to ran the decomposition for.
#' Default is set to be the range of all observed time points.
#' User can set it to be a shorter interval than the observed range.
#' Input for \code{\link{tempted}}.
#' @param resolution Number of time points to evaluate the value of the temporal loading function.
#' Default is set to 101. It does not affect the subject or feature loadings. Input for \code{\link{tempted}}.
#' @param maxiter Maximum number of iteration. Default is 20. Input for \code{\link{tempted}}.
#' @param epsilon Convergence criteria for difference between iterations. Default is 1e-4. Input for \code{\link{tempted}}.
#' @param r.svd The number of ranks in the mean structure. Default is 1. Input for \code{svd_centraliz}
#' @param pct.ratio The percent of features to sum up. Default is 0.05, i.e. 5%.
#' Input for \code{\link{ratio_feature}}.
#' @param absolute \code{absolute = TRUE} means features are ranked by the absolute value of feature loadings,
#' and the top \code{pct.ratio} percent of features are picked.
#' \code{absolute = FALSE} means features are ranked by the original value of feature loadings,
#' and the top and bottom \code{pct.ratio} percent of features are picked.
#' Then ratio is taken as the abundance of the features with positive loading
#' over the abundance of the features with negative loading.
#' Input for \code{\link{ratio_feature}}.
#' @param pct.aggregate The percent of features to aggregate,
#' features ranked by absolute value of the feature loading of each component.
#' Default is 1, which means 100% of features are aggregated.
#' Setting \code{pct.aggregate=0.01} means top 1% of features is aggregated,
#' where features are ranked in absolute value of feature loading of each component.
#' Input for \code{\link{aggregate_feature}}.
#' @param contrast A matrix choosing how components are combined,
#' each column is a contrast of length r and used to calculate the linear combination of
#' the feature loadings of r components.
#' Input for \code{\link{ratio_feature}} and Input for \code{\link{aggregate_feature}}.
#' @return A list including all the input and output of functions \code{\link{format_tempted}}, \code{svd_centralize()}, \code{\link{tempted}},
#' \code{\link{ratio_feature}}, and \code{\link{aggregate_feature}}.
#' \item{input}{All the input options of function \code{tempted_all()}.}
#' \item{datlist_raw}{}
#' \item{datlist}{}
#' \item{mean_svd}{}
#' \item{A.hat}{Subject loading, a subject by r matrix.}
#' \item{B.hat}{Feature loading, a feature by r matrix.}
#' \item{Phi.hat}{Temporal loading function, a resolution by r matrix.}
#' \item{time.Phi}{The time points where the temporal loading function is evaluted.}
#' \item{Lambda}{Eigen value, a length r vector.}
#' \item{r.square}{Variance explained by each component.
#' This is the R-squared of the linear regression of the vectorized temporal tensor against the vectorized low-rank reconstruction using individual components.}
#' \item{accum.r.square}{Variance explained by the first few components accumulated.
#' This is the R-squared of the linear regression of the vectorized temporal tensor against the vectorized low-rank reconstruction using the first few components.}
#' \item{metafeature.ratio}{The log ratio abundance of the top over bottom ranking features.
#' It is a data.frame with five columns: "value" for the log ratio values,
#' "subID" for the subject ID, and "timepoint" for the time points,
#' and "PC" indicating which component was used to construct the meta feature.}
#' \item{toppct.ratio}{A matrix of TRUE/FALSE indicating which features are ranked top in each component (and contrast)
#' and used as the numerator of the log ratio.}
#' \item{bottompct.ratio}{A matrix of TRUE/FALSE indicating which features are ranked bottom in each component (and contrast)
#' and used as the denominator of the log ratio.}
#' #' \item{metafeature.aggregate}{The meta feature obtained by aggregating the observed temporal tensor.
#' It is a data.frame with four columns: "value" for the meta feature values,
#' "subID" for the subject ID, "timepoint" for the time points,
#' and "PC" indicating which component was used to construct the meta feature.}
#' \item{toppct.aggregate}{A matrix of TRUE/FALSE indicating which features are aggregated in each component and contrast.}
#' \item{contrast}{The contrast used to linearly combine the components.
#' It is from the input parameter \code{contrast}.}
#' @examples
#' # for preprocessed data that do not need to be transformed
#'
#' res_processed <- tempted_all(processed_table,
#'                              meta_table$day_of_life,
#'                              meta_table$studyid,
#'                              threshold=1,
#'                              transform="none",
#'                              r=2,
#'                              smooth=1e-5)
#'
#' # for count data that will have pseudo_count added and clr transformed
#'
#' res_count <- tempted_all(count_table,
#'                          meta_table$day_of_life,
#'                          meta_table$studyid,
#'                          threshold=0.95,
#'                          transform="clr",
#'                          pseudo_count=0.5,
#'                          r=2,
#'                          smooth=1e-5)
#'
#' # for proportional data that will have pseudo_count added and clr transformed
#'
#' res_proportion <- tempted_all(count_table/rowSums(count_table),
#'                               meta_table$day_of_life,
#'                               meta_table$studyid,
#'                               threshold=0.95,
#'                               transform="clr",
#'                               pseudo_count=NULL,
#'                               r=2,
#'                               smooth=1e-5)
#'
#' # plot the temporal loading and subject trajectories grouped by delivery mode
#'
#' plot_time_loading(res_proportion, r=2)
#'
#' group <- unique(meta_table[,c("studyid", "delivery")])
#'
#' plot_metafeature(res_proportion$metafeature.aggregate, group, bws=30)
#' @md
tempted_all <- function(feature_table, time_point, subjectID,
                        threshold=0.95, pseudo_count=NULL, transform="clr",
                        r = 3, smooth=1e-6,
                        interval = NULL, resolution = 51,
                        maxiter=20, epsilon=1e-4,
                        r.svd=1,
                        pct.ratio=0.05, absolute=FALSE,
                        pct.aggregate=1, contrast=NULL){
  datlist <- format_tempted(feature_table=feature_table, time_point=time_point, subjectID=subjectID,
                            threshold=threshold, pseudo_count=pseudo_count, transform=transform)
  datlist_raw <- format_tempted(feature_table=feature_table, time_point=time_point, subjectID=subjectID,
                                threshold=threshold, pseudo_count=pseudo_count, transform="none")
  mean_svd <- svd_centralize(datlist, r.svd)
  res_tempted <- tempted(datlist=mean_svd$datlist, r = r, smooth=smooth,
                         interval = interval, resolution = resolution,
                         maxiter=maxiter, epsilon=epsilon)
  res_ratio <- ratio_feature(res_tempted=res_tempted, datlist=datlist_raw,
                             pct=pct.ratio, absolute=absolute, contrast=contrast)
  res_aggfeat <- aggregate_feature(res_tempted=res_tempted, mean_svd=mean_svd, datlist=datlist,
                                   pct=pct.aggregate, contrast=contrast)
  res_all <- list(input=as.list(match.call()))
  res_all$datlist_raw <- datlist_raw
  res_all$datlist <- datlist
  res_all <- append(res_all, res_tempted)
  res_all$metafeature.ratio <- res_ratio$metafeature.ratio
  res_all$toppct.ratio <- res_ratio$toppct
  res_all$bottompct.ratio <- res_ratio$toppct
  res_all$metafeature.aggregate <- res_aggfeat$metafeature.aggregate
  res_all$toppct.aggregate <- res_aggfeat$toppct
  res_all$contrast <- contrast
  return(res_all)
}



#' @title Plot nonparametric smoothed mean and error bands of features versus time
#' @description This is a handy function to plot the smoothed mean and error bands for multiple features.
#' @param feature_mat A sample by feature matrix. Each feature will be plotted separately as a facet.
#' The features can be original features, meta features, log ratios, or any variables of interest.
#' @param time_vec A vector of time points matched to the rows of \code{feature_mat}.
#' @param group_vec A vector of factor variable indicating the group membership
#' of samples matched to the rows of \code{feature_mat}.
#' @param coverage The coverage rate for the error band. Default is 0.95.
#' @param bws The smoothness parameter for the smoothing lines and error bands.
#' A larger value means a smoother line.
#' Default is NULL and calculated by function \code{np::npreg()}.
#' @param nrow The number of rows to plot the features used in function \code{ggplot2::facet_wrap()}.
#' @return A ggplot2 object.
#' @examples
#' # plot the summary of selected features
#'
#' feat_names <- c("OTU4447072", "OTU4467447")
#'
#' proportion_table <- count_table/rowSums(count_table)
#'
#' plot_feature_summary(proportion_table[,feat_names],
#'                      meta_table$day_of_life,
#'                      meta_table$delivery,
#'                      bws=30)
#' @md
plot_feature_summary <- function(feature_mat, time_vec, group_vec,
                                   coverage=0.95, bws=NULL, nrow=1){
  nfeature <- ncol(feature_mat)
  if(class(group_vec)!='factor') group_vec <- as.factor(group_vec)
  group_level <- levels(group_vec)
  time_all <- NULL
  mean_all <- NULL
  merr_all <- NULL
  feature_all <- NULL
  group_all <- NULL
  if(is.null(colnames(feature_mat))){stop('feature_mat needs to have column names!')}
  CI_length <- -qnorm((1-coverage)/2)
  for (jj in 1:nfeature){
    for (ii in 1:length(group_level)){
      ind <- group_vec==group_level[ii]
      if(is.null(bws)){
        model.np <- npreg(feature_mat[ind,jj]~time_vec[ind],
                          regtyle="ll", bwmethod="cv.aic")
      }else{
        model.np <- npreg(feature_mat[ind,jj]~time_vec[ind], bws=bws,
                          regtyle="ll", bwmethod="cv.aic")
      }
      time_eval <- as.vector(t(model.np$eval))
      mean_eval <- model.np$mean[order(time_eval)]
      merr_eval <- model.np$merr[order(time_eval)]
      time_eval <- sort(time_eval)

      time_all <- c(time_all, time_eval)
      mean_all <- c(mean_all, mean_eval)
      merr_all <- c(merr_all, merr_eval)
      feature_all <- c(feature_all,
                       rep(colnames(feature_mat)[jj], length(time_eval)))
      group_all <- c(group_all,
                     rep(group_level[ii], length(time_eval)))
    }
  }
  group_all <- factor(group_all, levels=group_level)
  tab_summary <- data.frame(time=time_all, mean=mean_all, merr=merr_all,
                            group=group_all, feature=feature_all)

  p_summary <- ggplot(data=tab_summary,
                      aes(x=time, y=mean, group=group, color=group)) +
    geom_line() +
    geom_ribbon(aes(ymin=mean-CI_length*merr, ymax=mean+CI_length*merr,
                    color=group, fill=group), linetype=2, alpha=0.3) +
    ylab(paste0('mean +/- ', round(CI_length,2), '*se')) + facet_wrap(.~feature, scales="free", nrow=nrow)
  return(p_summary)
}



#' @title Plot nonparametric smoothed mean and error bands of meta features versus time
#' @description This function plot the smoothed mean and error band of meta features
#' grouped by a factor variable provided by the user.
#' @param metafeature It can be \code{metafeature.ratio} from the output of \code{\link{ratio_feature}} and \code{tempted_all()},
#' \code{metafeature.aggregate} from the output of \code{\link{ratio_feature}} and \code{tempted_all()},
#' or \code{metafeature.aggregate.est} from the output of \code{\link{ratio_feature}}.
#' @param group A subject by 2 data.frame with the first column for subject ID and second column for group membership.
#' @param coverage The coverage rate for the error band. Default is 0.95.
#' @param bws The smoothness parameter for the smoothing lines and error bands.
#' A larger value means a smoother line.
#' Default is NULL and calculated by function \code{np::npreg()}.
#' @param nrow The number of rows to plot the features used in function \code{ggplot2::facet_wrap()}.
#' @return A ggplot2 object.
#' @seealso Examples can be found in \code{\link{tempted_all}}, \code{\link{ratio_feature}} and \code{\link{aggregate_feature}}.
#' @md
plot_metafeature <- function(metafeature, group,
                             coverage=0.95, bws=NULL, nrow=1){
  colnames(group) <- c("subID", "group")
  tab_feat_ratio <- merge(metafeature, group, by="subID")
  ## summed up, by mean and sd
  reshape_feat_ratio <- reshape(tab_feat_ratio,
                                idvar=c("subID","timepoint") ,
                                v.names=c("value"), timevar="PC",
                                direction="wide")
  CC <- grep('value',colnames(reshape_feat_ratio))
  colnames(reshape_feat_ratio)[CC] <- substr(colnames(reshape_feat_ratio)[CC], start=7, stop=100)
  feature_mat_ratio <- reshape_feat_ratio[,CC]
  colnames(feature_mat_ratio)
  time_vec_ratio <- reshape_feat_ratio$timepoint
  group_vec_ratio <- factor(reshape_feat_ratio$group)
  p_feat_summary <- plot_feature_summary(feature_mat_ratio,
                                               time_vec_ratio,
                                               group_vec_ratio, bws=bws, nrow=nrow)
  return(p_feat_summary)
}


#' @title Plot the temporal loading functions
#' @description This function uses \code{ggplot2::geom_line()} in ggplot2 to plot the temporal loading functions from \code{\link{tempted}}.
#' @param res Output of function \code{\link{tempted}}.
#' @param r The number of components to plot. By default all the components estimated by \code{\link{tempted}} will be plotted.
#' @param ... Arguments to put in \code{ggplot2::geom_line(aes(...))}.
#' @return An ggplot2 object.
#' @seealso Examples can be found in \code{\link{tempted_all}} and \code{\link{tempted}}.
#' @md
plot_time_loading <- function(res, r=NULL, ...){
  Phi.data <- res$Phi.hat
  if(is.null(r)) r <- ncol(Phi.data)
  Phi.data <- Phi.data[,1:r]
  ntime <- nrow(Phi.data)
  Phi.data <- data.frame(time=res$time.Phi, value=as.vector(Phi.data),
                         component=as.factor(as.vector(t(matrix(rep(1:r,ntime),r,)))))
  ptime <- ggplot(data=Phi.data, aes(x=time, y=value, color=component)) + geom_line(aes(...))
  return(ptime)
}


#' Meta data table from the ECAM data
#' @format A data.frame with rows matching with data.frame \code{count_table} and \code{processed_table}
#' and three variables:
#' \describe{
#' \item{studyid}{character denoting the subject ID of the infants.}
#' \item{delivery}{character denoting the delivery mode of the infants.}
#' \item{day_of_life}{character denoting the age of infants measured in days when microbiome sample was taken.}
#' }
#' @source Bokulich, Nicholas A., et al. "Antibiotics, birth mode, and diet shape microbiome maturation during early life." Science translational medicine 8.343 (2016): 343ra82-343ra82.
#' @md
"meta_table"


#' OTU read count table from the ECAM data
#' @format A data.frame with rows matching with data.frame \code{meta_table}
#' and columns representing microbial features (i.e. OTUs). Each entry is a read count.
#' @source Bokulich, Nicholas A., et al. "Antibiotics, birth mode, and diet shape microbiome maturation during early life." Science translational medicine 8.343 (2016): 343ra82-343ra82.
#' @md
"count_table"


#' Central-log-ratio (clr) transformed OTU table from the ECAM data
#' @format A data.frame with rows matching with data.frame \code{meta_table}
#' and columns representing microbial features (i.e. OTUs).
#' Entries do not need to be transformed, and will be directly used by \code{tempted}.
#' This data.frame is used to illustrate how \code{tempted} can be used for
#' general form of multivariate longitudinal data already preprocessed by user.
#' @source Bokulich, Nicholas A., et al. "Antibiotics, birth mode, and diet shape microbiome maturation during early life." Science translational medicine 8.343 (2016): 343ra82-343ra82.
#' @md
"processed_table"

